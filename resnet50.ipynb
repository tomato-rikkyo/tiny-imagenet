{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2.3でメモリを指定及び節約して使うためのおまじない。\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(219)\n",
    "random.seed(219)\n",
    "tf.random.set_seed(219)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dirpath = \"/workdir/taki_lab/tiny-imagenet-200/\"\n",
    "train_dir = os.path.join(dirpath, \"train\")\n",
    "val_dir = os.path.join(dirpath, \"val\")\n",
    "test_dir = os.path.join(dirpath, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "N_TRAIN = 100000\n",
    "N_VAL = 10000\n",
    "INPUT_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                              rotation_range=10,\n",
    "                              width_shift_range=0.1,\n",
    "                              height_shift_range=0.1,\n",
    "                              shear_range=0.1,\n",
    "                              zoom_range=0.1,\n",
    "                              fill_mode=\"nearest\")\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_gen.flow_from_directory(train_dir,\n",
    "                                               target_size=INPUT_SIZE,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               class_mode=\"categorical\")\n",
    "\n",
    "val_generator = val_gen.flow_from_directory(val_dir,\n",
    "                                           target_size=INPUT_SIZE,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model\n",
    "\n",
    "def bottleneck(x, in_ch, out_ch, strides=1):\n",
    "    params = {\n",
    "        \"kernel_initializer\": \"he_normal\",\n",
    "        \"use_bias\": False\n",
    "    }\n",
    "    \n",
    "    inter_ch = out_ch // 4\n",
    "    h1 = layers.BatchNormalization()(x)\n",
    "    h1 = layers.ReLU()(h1)\n",
    "    h1 = layers.Conv2D(inter_ch, kernel_size=1, strides=strides, padding=\"valid\", **params)(h1)\n",
    "    h1 = layers.BatchNormalization()(h1)\n",
    "    h1 = layers.ReLU()(h1)\n",
    "    h1 = layers.Conv2D(inter_ch, kernel_size=3, strides=1, padding=\"same\", **params)(h1)\n",
    "    h1 = layers.BatchNormalization()(h1)\n",
    "    h1 = layers.ReLU()(h1)\n",
    "    h1 = layers.Conv2D(out_ch, kernel_size=1, strides=1, padding=\"valid\", **params)(h1)\n",
    "    \n",
    "    if in_ch != out_ch:\n",
    "        h2 = layers.BatchNormalization()(x)\n",
    "        h2 = layers.Conv2D(out_ch, kernel_size=1, strides=strides, padding=\"same\", **params)(h2)\n",
    "    else:\n",
    "        h2 = x\n",
    "    \n",
    "    h = layers.Add()([h1, h2])\n",
    "\n",
    "    return h\n",
    "\n",
    "def functional_resnet50(input_shape, output_size):\n",
    "    inputs = layers.Input(input_shape)\n",
    "    x = layers.BatchNormalization()(inputs)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Conv2D(64, 7, 2, padding=\"same\", kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "    x = layers.MaxPool2D(3, 2, padding=\"same\")(x)\n",
    "    \n",
    "    x = bottleneck(x, 64, 256)\n",
    "    x = bottleneck(x, 256, 256)\n",
    "    x = bottleneck(x, 256, 256)\n",
    "    \n",
    "    x = layers.Conv2D(512, kernel_size=1, strides=2)(x)\n",
    "    x = bottleneck(x, 512, 512)\n",
    "    x = bottleneck(x, 512, 512)\n",
    "    x = bottleneck(x, 512, 512)\n",
    "    x = bottleneck(x, 512, 512)\n",
    "    \n",
    "    x = layers.Conv2D(1024, kernel_size=1, strides=2, use_bias=False)(x)\n",
    "    x = bottleneck(x, 1024, 1024)\n",
    "    x = bottleneck(x, 1024, 1024)\n",
    "    x = bottleneck(x, 1024, 1024)\n",
    "    x = bottleneck(x, 1024, 1024)\n",
    "    x = bottleneck(x, 1024, 1024)\n",
    "    x = bottleneck(x, 1024, 1024)\n",
    "    \n",
    "    x = layers.Conv2D(2048, kernel_size=1, strides=2, use_bias=False)(x)\n",
    "    x = bottleneck(x,2048, 2048)\n",
    "    x = bottleneck(x,2048, 2048)\n",
    "    x = bottleneck(x,2048, 2048)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "    outputs = layers.Dense(output_size, activation=\"softmax\", kernel_initializer=\"he_normal\")(x)\n",
    "    return Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/tomato-ai/tiny-image-net\" target=\"_blank\">https://app.wandb.ai/tomato-ai/tiny-image-net</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/tomato-ai/tiny-image-net/runs/24ixnyit\" target=\"_blank\">https://app.wandb.ai/tomato-ai/tiny-image-net/runs/24ixnyit</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 27.1132 - acc: 0.0047WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 27.1062 - acc: 0.0047WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - 640s 205ms/step - loss: 27.1062 - acc: 0.0047 - val_loss: 5.3039 - val_acc: 0.0050\n",
      "Epoch 2/1000\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 5.3055 - acc: 0.0045WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 5.3055 - acc: 0.0045WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - 631s 202ms/step - loss: 5.3055 - acc: 0.0045 - val_loss: 5.3037 - val_acc: 0.0050\n",
      "Epoch 3/1000\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 5.3056 - acc: 0.0046WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 5.3056 - acc: 0.0046WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - 631s 202ms/step - loss: 5.3056 - acc: 0.0046 - val_loss: 5.3032 - val_acc: 0.0050\n",
      "Epoch 4/1000\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 5.3057 - acc: 0.0043WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 5.3057 - acc: 0.0043WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - 642s 205ms/step - loss: 5.3057 - acc: 0.0043 - val_loss: 5.3026 - val_acc: 0.0050\n",
      "Epoch 5/1000\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 5.3051 - acc: 0.0048WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 5.3051 - acc: 0.0048WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - 633s 202ms/step - loss: 5.3051 - acc: 0.0048 - val_loss: 5.3038 - val_acc: 0.0050\n",
      "Epoch 6/1000\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3124/3125 [============================>.] - ETA: 0s - loss: 5.3055 - acc: 0.0047WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - ETA: 0s - loss: 5.3055 - acc: 0.0047WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "3125/3125 [==============================] - 635s 203ms/step - loss: 5.3055 - acc: 0.0047 - val_loss: 5.3041 - val_acc: 0.0050\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# wandbの設定\n",
    "wandb.init(project=\"tiny-image-net\", name=\"resnet50_pre-activation\")\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01\n",
    "config.lr_factor = 0.5\n",
    "config.optimaizer = \"Adam\"\n",
    "config.batch_size = BATCH_SIZE\n",
    "config.input_size = INPUT_SIZE\n",
    "config.rotation_range = 10\n",
    "config.width_shift_range = 0.1\n",
    "config.height_shift_range = 0.1\n",
    "config.shear_range = 0.1\n",
    "config.zoom_range = 0.1\n",
    "\n",
    "# 最適化設定\n",
    "model = functional_resnet50((224, 224, 3), 200)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"acc\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5)\n",
    "\n",
    "callback_list = [early_stopping, reduce_lr, WandbCallback()]\n",
    "\n",
    "\n",
    "# 学習\n",
    "history = model.fit(train_generator,\n",
    "    steps_per_epoch=N_TRAIN//BATCH_SIZE,\n",
    "    epochs=1000,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=N_VAL//BATCH_SIZE,\n",
    "    use_multiprocessing=True,\n",
    "    workers=24,\n",
    "    callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
