{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 2.3でメモリを指定及び節約して使うためのおまじない。\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[1], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[1], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dirpath = \"/workdir/taki_lab/tiny-imagenet-200/\"\n",
    "train_dir = os.path.join(dirpath, \"train\")\n",
    "val_dir = os.path.join(dirpath, \"val\")\n",
    "test_dir = os.path.join(dirpath, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "N_TRAIN = 100000\n",
    "N_VAL = 10000\n",
    "INPUT_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                              rotation_range=10,\n",
    "                              width_shift_range=0.1,\n",
    "                              height_shift_range=0.1,\n",
    "                              shear_range=0.1,\n",
    "                              zoom_range=0.1,\n",
    "                              fill_mode=\"nearest\")\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_gen.flow_from_directory(train_dir,\n",
    "                                               target_size=INPUT_SIZE,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               class_mode=\"categorical\")\n",
    "\n",
    "val_generator = val_gen.flow_from_directory(val_dir,\n",
    "                                           target_size=INPUT_SIZE,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           class_mode=\"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, Model\n",
    "\n",
    "class BRC(Model):\n",
    "    def __init__(self, filters, kernel_size, strides):\n",
    "        super().__init__()\n",
    "        params = {\n",
    "            \"filters\":filters,\n",
    "            \"kernel_size\":kernel_size,\n",
    "            \"strides\":strides,\n",
    "            \"padding\":\"same\",\n",
    "            \"use_bias\":True,\n",
    "            \"kernel_initializer\":\"he_normal\"\n",
    "        }\n",
    "        \n",
    "        self.layers_ = [\n",
    "            layers.BatchNormalization(),\n",
    "            layers.ReLU(),\n",
    "            layers.Conv2D(**params)\n",
    "        ]\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        for layer in self.layers_:\n",
    "            inputs = layer(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(Model):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "        self.layers_ = [\n",
    "            BRC(64, 3, 1),\n",
    "            BRC(64, 3, 1),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            BRC(128, 3, 1),\n",
    "            BRC(128, 3, 1),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            BRC(256, 3, 1),\n",
    "            BRC(256, 3, 1),\n",
    "            BRC(256, 3, 1),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            BRC(256, 3, 1),\n",
    "            BRC(256, 3, 1),\n",
    "            BRC(256, 3, 1),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            BRC(512, 3, 1),\n",
    "            BRC(512, 3, 1),\n",
    "            BRC(512, 3, 1),\n",
    "            layers.MaxPool2D((2,2)),\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dense(4096, kernel_initializer=\"he_normal\"),\n",
    "            layers.Dense(output_size, activation=\"softmax\")\n",
    "        ]\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        for layer in self.layers_:\n",
    "            inputs = layer(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/tomato-ai/tiny-image-net\" target=\"_blank\">https://app.wandb.ai/tomato-ai/tiny-image-net</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/tomato-ai/tiny-image-net/runs/3mz6zdef\" target=\"_blank\">https://app.wandb.ai/tomato-ai/tiny-image-net/runs/3mz6zdef</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.2 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "  45/3125 [..............................] - ETA: 13:51 - loss: 428.6401 - acc: 0.0083"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# wandbの設定\n",
    "wandb.init(project=\"tiny-image-net\", name=\"resnet50_finetuning_aug\")\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.01\n",
    "config.lr_factor = 0.5\n",
    "config.optimaizer = \"Adam\"\n",
    "config.batch_size = BATCH_SIZE\n",
    "config.input_size = INPUT_SIZE\n",
    "config.rotation_range = 10\n",
    "config.width_shift_range = 0.1\n",
    "config.height_shift_range = 0.1\n",
    "config.shear_range = 0.1\n",
    "config.zoom_range = 0.1\n",
    "\n",
    "# 最適化設定\n",
    "\n",
    "model = VGG16(200)\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.01),\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"acc\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5)\n",
    "\n",
    "callback_list = [early_stopping, reduce_lr, WandbCallback()]\n",
    "\n",
    "\n",
    "# 学習\n",
    "history = model.fit(train_generator,\n",
    "    steps_per_epoch=N_TRAIN//BATCH_SIZE ,\n",
    "    epochs=1000,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=N_VAL//BATCH_SIZE,\n",
    "    callbacks=callback_list,\n",
    "    use_multiprocessing=True,\n",
    "                   workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16_ft+da\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               205000    \n",
      "=================================================================\n",
      "Total params: 15,445,000\n",
      "Trainable params: 15,445,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "from keras.applications import VGG16\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "vgg16_base = VGG16(weights=None, include_top=False,\n",
    "                             input_tensor=Input(shape=(224,224,3)))\n",
    "\n",
    "x = vgg16_base.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "output_tensor = layers.Dense(200, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=vgg16_base.input, outputs=output_tensor, name=\"vgg16_ft+da\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/tomato-ai/tiny-image-net\" target=\"_blank\">https://app.wandb.ai/tomato-ai/tiny-image-net</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/tomato-ai/tiny-image-net/runs/2jfzf24j\" target=\"_blank\">https://app.wandb.ai/tomato-ai/tiny-image-net/runs/2jfzf24j</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.9.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1562/1562 [==============================] - 721s 461ms/step - loss: 2.3598 - acc: 0.4613 - val_loss: 1.8267 - val_acc: 0.5608\n",
      "Epoch 2/100\n",
      "1562/1562 [==============================] - 728s 466ms/step - loss: 1.7076 - acc: 0.5771 - val_loss: 1.6977 - val_acc: 0.5869\n",
      "Epoch 3/100\n",
      "1562/1562 [==============================] - 733s 469ms/step - loss: 1.5530 - acc: 0.6097 - val_loss: 1.6451 - val_acc: 0.5982\n",
      "Epoch 4/100\n",
      "1562/1562 [==============================] - 740s 474ms/step - loss: 1.4495 - acc: 0.6317 - val_loss: 1.6209 - val_acc: 0.6049\n",
      "Epoch 5/100\n",
      "1562/1562 [==============================] - 739s 473ms/step - loss: 1.3705 - acc: 0.6472 - val_loss: 1.6101 - val_acc: 0.6075\n",
      "Epoch 6/100\n",
      "1562/1562 [==============================] - 743s 476ms/step - loss: 1.3071 - acc: 0.6606 - val_loss: 1.5845 - val_acc: 0.6163\n",
      "Epoch 7/100\n",
      "1562/1562 [==============================] - 740s 473ms/step - loss: 1.2522 - acc: 0.6714 - val_loss: 1.5834 - val_acc: 0.6157\n",
      "Epoch 8/100\n",
      "  67/1562 [>.............................] - ETA: 12:00 - loss: 1.1698 - acc: 0.6905"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# wandbの設定\n",
    "wandb.init(project=\"tiny-image-net\", name=\"resnet50_finetuning_aug\")\n",
    "config = wandb.config\n",
    "config.learning_rate = 0.0001\n",
    "config.lr_factor = 0.5\n",
    "config.optimaizer = \"RMSprop\"\n",
    "config.batch_size = BATCH_SIZE\n",
    "config.input_size = INPUT_SIZE\n",
    "config.rotation_range = 10\n",
    "config.width_shift_range = 0.1\n",
    "config.height_shift_range = 0.1\n",
    "config.shear_range = 0.1\n",
    "config.zoom_range = 0.1\n",
    "\n",
    "# 最適化設定\n",
    "model.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4),\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"acc\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-5)\n",
    "\n",
    "callback_list = [early_stopping, reduce_lr, WandbCallback()]\n",
    "\n",
    "\n",
    "# 学習\n",
    "history = model.fit(train_generator,\n",
    "    steps_per_epoch=N_TRAIN//BATCH_SIZE ,\n",
    "    epochs=100,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=N_VAL//BATCH_SIZE,\n",
    "    callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
